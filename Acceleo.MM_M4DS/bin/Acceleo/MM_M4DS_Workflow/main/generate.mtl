[comment encoding = UTF-8 /]
[**
 * The documentation of the module generate.
 */]
[module generate('https://www.example.org/workflow', 'https://www.example.org/contract', 'http://www.example.org/Library', 'https://www.example.org/environment')]

[import Acceleo::MM_M4DS_contract::main::generate/]


[**
 * The documentation of the template generateElement.
 * @param aWorkflow
 */]

[template public generateWorkflow(aWorkflow : Workflow)]
[comment @main/]
[generateEnvironment(aWorkflow)/]
[generateDataProcessing(aWorkflow)/]
[generateContractsDataProcessing(aWorkflow)/]
[generateTransformationsDataProcessing(aWorkflow)/]
[/template]


[template public generateDataProcessing(aWorkflow : Workflow)]
[file ('dataProcessing_Job_'+aWorkflow.name.replaceAll('[(),\\s]+', '_')+'.py', false, 'UTF-8')]
import pandas as pd
import numpy as np
import functions.contract_invariants as contract_invariants
import functions.contract_pre_post as contract_pre_post
import functions.data_transformations as data_transformations
from helpers.enumerations import Belong, Operator, Operation, SpecialType, DataType, DerivedType, Closure, FilterType
from helpers.logger import set_logger

[comment generate the imports and the class/]
def generateWorkflow():
[for (d : DataProcessing | aWorkflow.dataprocessing)]													[comment OPEN traverse all the DataProcessings/]
	[if (d.incoming = null and d.outgoing=null)]														[comment OPEN checks if the DataProcessing is isolated (no links)/]
		[if (d.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]				[comment OPEN checks if the dataPrcessingDefinition is of type DataProcessingDefinition/]
			[let dpd : Library::DataProcessingDefinition = d.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]	[comment OPEN assigns the variable to be of type DataProcessingDefinition/]
				[if (dpd.oclIsKindOf(Library::Transformation))]											[comment OPEN checks if the dataPrcessingDefinition is of type Transformation/]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]		[comment OPEN assigns the variable to be of type Transformation/]
	#-----------------New DataProcessing-----------------
						[for (dc : DataDictionary | d.inputPort)]										[comment OPEN read all the input datasets and store them/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dc.fileName/]', sep = '[dc.columnDelimiter/]')
							
						[/for]																			[comment CLOSE read all the input datasets and store them/]
						[for (c : Contract | d.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
							[if (c.contract.type=ContractType::PRECONDITION)]							[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
							[/if]																		[comment CLOSE checks if the if the contract is a Precondition/]
						[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
	[generateCallTransformation(d, d)/]
						[for (c : Contract | d.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
							[if (c.contract.type=ContractType::POSTCONDITION)]							[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
							[/if]																		[comment CLOSE checks if the if the contract is a Postcondition/]
						[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
						[for (c : Contract | d.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
							[if (c.contract.type=ContractType::INVARIANT)]								[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
							[/if]																		[comment CLOSE checks if the if the contract is a Invariant/]
						[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
					[/let]																				[comment CLOSE assigns the variable to be of type Transformation/]
				[elseif (dpd.oclIsKindOf(Library::Job))]												[comment OPEN checks if the dataPrcessingDefinition is of type Job/]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]								[comment OPEN assigns the variable to be of type Job/]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | d._in)]													[comment OPEN traverse all the input arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]										[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]			[comment OPEN assigns the variables to be a DataDictionary/]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_in.fileName/]'

	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_in.columnDelimiter/]'
								[/let]																	[comment CLOSE assigns the variables to be a DataDictionary/]
							[/if]																		[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																			[comment CLOSE traverse all the input arguments/]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | d.out)]													[comment OPEN traverse all the output arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]										[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]			[comment OPEN assigns the variables to be a DataDictionary/]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_out.fileName/]'
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_out.columnDelimiter/]'
								[/let]																	[comment CLOSE assigns the variables to be a DataDictionary/]
							[/if]																		[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																			[comment CLOSE traverse all the output arguments/]

						[for (dataP : DataProcessing | libJ.workflow.dataprocessing)]					[comment OPEN traverse the dataProcessings in the workflow/]
							[if (dataP.incoming = null and dataP.outgoing=null)]						[comment OPEN checks if the dataProcessing is isolated/]
								[for (c : Contract | dataP.contract)]									[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::PRECONDITION)]					[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Precondition/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
	[generateCallTransformation(dataP, d)/]
								[for (c : Contract | dataP.contract)]									[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::POSTCONDITION)]					[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Postcondition/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
								[for (c : Contract | dataP.contract)]									[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::INVARIANT)]						[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Invariant/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
							[/if]																		[comment CLOSE checks if the dataProcessing is isolated/]
						[/for]																			[comment CLOSE traverse the dataProcessings in the workflow/]
						[for (dataP : DataProcessing | libJ.workflow.dataprocessing)]					[comment OPEN traverse the dataProcessings in the workflow/]
							[if (dataP.incoming = null and dataP.outgoing<>null)]						[comment OPEN checks that the dataProcessing is not isolated and is the first/]
[callModifiedRecursiveTemplate(dataP, d)/]
							[/if]																		[comment CLOSE checks that the dataProcessing is not isolated and is the first/]
						[/for]																			[comment CLOSE traverse the dataProcessings in the workflow/]
					[/let]																				[comment CLOSE assigns the variable to be of type Job/]
				[/if]																					[comment CLOSE checks if the dataPrcessingDefinition is of type Transformation or Job/]
			[/let]																														[comment CLOSE assigns the variable to be of type DataProcessingDefinition/]
		[/if]																							[comment CLOSE checks if the dataPrcessingDefinition is of type DataProcessingDefinition/]
	[/if]																								[comment CLOSE checks if the DataProcessing is isolated (no links)/]
[/for]																									[comment CLOSE traverse all the DataProcessings/]
[for (d : DataProcessing | aWorkflow.dataprocessing)]													[comment OPEN traverse all the DataProcessings/]
	[if (d.incoming = null and d.outgoing<>null)]														[comment OPEN checks if the DataProcessing is the first (no input link, an output link)/]
		[if (d.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]				[comment OPEN checks if the dataPrcessingDefinition is of type DataProcessingDefinition/]
			[let dpd : Library::DataProcessingDefinition = d.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]	[comment OPEN assigns the variable to be of type DataProcessingDefinition/]
				[if (dpd.oclIsKindOf(Library::Transformation))]											[comment OPEN checks if the dataPrcessingDefinition is of type Transformation/]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]		[comment OPEN assigns the variable to be of type Transformation/]
[callRecursiveTemplate(d)/]
					[/let]																				[comment CLOSE assigns the variable to be of type Transformation/]
				[elseif (dpd.oclIsKindOf(Library::Job))]												[comment OPEN checks if the dataPrcessingDefinition is of type Job/]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]								[comment OPEN assigns the variable to be of type Job/]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | d._in)]													[comment OPEN traverse all the input arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]										[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]			[comment OPEN assigns the variables to be a DataDictionary/]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_in.fileName/]'
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_in.columnDelimiter/]'
								[/let]																	[comment CLOSE assigns the variables to be a DataDictionary/]
							[/if]																		[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																			[comment CLOSE traverse all the input arguments/]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | d.out)]													[comment OPEN traverse all the output arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]										[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]			[comment OPEN assigns the variables to be a DataDictionary/]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_out.fileName/]'
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_out.columnDelimiter/]'
								[/let]																	[comment CLOSE assigns the variables to be a DataDictionary/]
							[/if]																		[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																			[comment CLOSE traverse all the output arguments/]

						[for (dataP : DataProcessing | libJ.workflow.dataprocessing)]					[comment OPEN traverse the dataProcessings in the workflow/]
							[if (dataP.incoming = null and dataP.outgoing=null)]						[comment OPEN checks if the dataProcessing is isolated/]
								[for (c : Contract | dataP.contract)]									[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::PRECONDITION)]					[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Precondition/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
	[generateCallTransformation(dataP, d)/]
								[for (c : Contract | dataP.contract)]									[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::POSTCONDITION)]					[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Postcondition/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
								[for (c : Contract | dataP.contract)]									[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::INVARIANT)]						[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Invariant/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
							[/if]																		[comment CLOSE checks if the dataProcessing is isolated/]
						[/for]																			[comment CLOSE traverse the dataProcessings in the workflow/]
						[for (dataP : DataProcessing | libJ.workflow.dataprocessing)]					[comment OPEN traverse the dataProcessings in the workflow/]
							[if (dataP.incoming = null and dataP.outgoing<>null)]						[comment OPEN checks that the dataProcessing is not isolated and is the first/]
[callModifiedRecursiveTemplate(dataP, d)/]
							[/if]																		[comment CLOSE checks that the dataProcessing is not isolated and is the first/]
						[/for]																			[comment CLOSE traverse the dataProcessings in the workflow/]
					[/let]																				[comment CLOSE assigns the variable to be of type Job/]
				[/if]																					[comment CLOSE checks if the dataPrcessingDefinition is of type Transformation or Job/]
			[/let]																													[comment CLOSE assigns the variable to be of type DataProcessingDefinition/]
		[/if]																							[comment CLOSE checks if the dataPrcessingDefinition is of type DataProcessingDefinition/]
	[/if]																								[comment CLOSE checks if the DataProcessing is the first (no input link, an output link)/]
[/for]																									[comment CLOSE traverse all the DataProcessings/]
set_logger("dataProcessing")
generateWorkflow()
[/file]
[/template]


[template public callModifiedRecursiveTemplate(dw : DataProcessing, d_or : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (dw.incoming <> null)]
	[if (dw.incoming.source.dataProcessingDefinition.name<>'split')]
	[dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/], sep=[dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)

	[else]
		[for (dc : DataDictionary | dw.inputPort)]								[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | dw.incoming.source.outputPort)]		[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName = dc_out.fileName)]									[comment OPEN checks that both datasets are the same/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dc.name.replaceAll('[(),\\s]+', '_')/], sep=[dc.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)

				[/if]															[comment CLOSE checks that both datasets are the same/]
			[/for]																[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																	[comment CLOSE read all the input datasets and store them/]
	[/if]
[else]
	[for (dc : DataDictionary | dw.inputPort)]									[comment OPEN read all the input datasets and store them/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dc.name.replaceAll('[(),\\s]+', '_')/], sep = [dc.name.replaceAll('[(),\\s]+', '_')/]_sep)
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dc.name.replaceAll('[(),\\s]+', '_')/])
	[/for]																		[comment CLOSE read all the input datasets and store them/]
[/if]
[for (c : Contract | dw.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::PRECONDITION)]							[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Precondition/]
[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
	[generateCallTransformation(dw, d_or)/]
[for (c : Contract | dw.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::POSTCONDITION)]							[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Postcondition/]
[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | dw.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::INVARIANT)]								[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Invariant/]
[/for]
[if (dw.outgoing<>null)]															[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = dw.outgoing.target.oclAsType(DataProcessing)]	[comment OPEN assigns the next DataProcessing to a variable/]
[callModifiedRecursiveTemplate(nextDp, d_or)/]			[comment Recursive call to this template to generate all the DataProcessings linked to the first/]
	[/let]																		[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																			[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public callRecursiveTemplate(d : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (d.incoming <> null)]
	[if (d.incoming.source.dataProcessingDefinition.name<>'split')]
	[d.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[d.inputPort->first().fileName/]', sep='[d.inputPort->first().columnDelimiter/]', index_col=0)

	[else]
		[for (dc : DataDictionary | d.inputPort)]								[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | d.incoming.source.outputPort)]		[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName = dc_out.fileName)]									[comment OPEN checks that both datasets are the same/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[dc_out.fileName/]', sep='[dc_out.columnDelimiter/]', index_col=0)

				[/if]															[comment CLOSE checks that both datasets are the same/]
			[/for]																[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																	[comment CLOSE read all the input datasets and store them/]
	[/if]
[else]
	[for (dc : DataDictionary | d.inputPort)]									[comment OPEN read all the input datasets and store them/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[dc.fileName/]', sep = '[dc.columnDelimiter/]')
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[dc.fileName/]')
	[/for]																		[comment CLOSE read all the input datasets and store them/]
[/if]
[for (c : Contract | d.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::PRECONDITION)]							[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Precondition/]
[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
	[generateCallTransformation(d, d)/]
[for (c : Contract | d.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::POSTCONDITION)]							[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Postcondition/]
[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | d.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::INVARIANT)]								[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Invariant/]
[/for]
[if (d.outgoing<>null)]															[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = d.outgoing.target.oclAsType(DataProcessing)]	[comment OPEN assigns the next DataProcessing to a variable/]
[callRecursiveTemplate(nextDp)/]			[comment Recursive call to this template to generate all the DataProcessings linked to the first/]
	[/let]																		[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																			[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public generateContractsDataProcessing(aWorkflow : Workflow)]
[file ('contracts_Job_'+aWorkflow.name.replaceAll('[(),\\s]+', '_')+'.py', false, 'UTF-8')]
import os

import pandas as pd
import numpy as np
import functions.contract_invariants as contract_invariants
import functions.contract_pre_post as contract_pre_post
from helpers.enumerations import Belong, Operator, Operation, SpecialType, DataType, DerivedType, Closure, FilterType
from helpers.logger import set_logger

[comment generate the imports and the class/]
def generateWorkflow():
[for (d : DataProcessing | aWorkflow.dataprocessing)]							[comment OPEN traverse all the DataProcessings/]
	[if (d.incoming = null and d.outgoing=null)]								[comment OPEN checks if the DataProcessing is isolated (no links)/]
	#-----------------New DataProcessing-----------------
		[if (d.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
			[let dpd : Library::DataProcessingDefinition = d.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
				[if (dpd.oclIsKindOf(Library::Transformation))]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
						[for (dc : DataDictionary | d.inputPort)]								[comment OPEN read all the input datasets and store them/]
	[dc.dataDictionaryDefinition.name/]=pd.read_csv('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dc.fileName/]', sep = '[dc.columnDelimiter/]')

						[/for]																	[comment CLOSE read all the input datasets and store them/]
						[for (dc : DataDictionary | d.outputPort)]								[comment OPEN read all the output datasets and store them/]
	if os.fileName.exists('[dc.fileName/]'):		#If the output DataDictionary exists, we store it
		[dc.dataDictionaryDefinition.name/]_df=pd.read_csv('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dc.fileName/]', sep = '[dc.columnDelimiter/]', sep = '[dc.columnDelimiter/]')

						[/for]																	[comment CLOSE read all the output datasets and store them/]
						[for (c : Contract | d.contract)]										[comment OPEN traverse all the contracts in the DataProcessing/]
							[if (c.contract.type=ContractType::PRECONDITION)]					[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
							[/if]																[comment CLOSE checks if the if the contract is a Precondition/]
						[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
						[for (c : Contract | d.contract)]										[comment OPEN traverse all the contracts in the DataProcessing/]
							[if (c.contract.type=ContractType::POSTCONDITION)]					[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
							[/if]																[comment CLOSE checks if the if the contract is a Postcondition/]
						[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
						[for (c : Contract | d.contract)]										[comment OPEN traverse all the contracts in the DataProcessing/]
							[if (c.contract.type=ContractType::INVARIANT)]						[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
							[/if]																[comment CLOSE checks if the if the contract is a Invariant/]
						[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
					[/let]
				[elseif (dpd.oclIsKindOf(Library::Job))]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | d._in)]
							[if (arg.oclIsKindOf(DataDictionary))]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_in.fileName/]'
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_in.columnDelimiter/]'
								[/let]
							[/if]
						[/for]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | d.out)]
							[if (arg.oclIsKindOf(DataDictionary))]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_out.fileName/]'
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_out.columnDelimiter/]'
								[/let]
							[/if]
						[/for]


						[for (dataP : DataProcessing | libJ.workflow.dataprocessing)]
							[if (dataP.incoming = null and dataP.outgoing=null)]
								[for (c : Contract | dataP.contract)]										[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::PRECONDITION)]					[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Precondition/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
								[for (c : Contract | dataP.contract)]										[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::POSTCONDITION)]					[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Postcondition/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
								[for (c : Contract | dataP.contract)]										[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::INVARIANT)]						[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Invariant/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
							[/if]
						[/for]
						[for (dataP : DataProcessing | libJ.workflow.dataprocessing)]
							[if (dataP.incoming = null and dataP.outgoing<>null)]
[callContractsModifiedRecursiveTemplate(dataP, d)/]
							[/if]
						[/for]
					[/let]
				[/if]
			[/let]
		[/if]
	[/if]																		[comment CLOSE checks if the DataProcessing is isolated (no links)/]
[/for]																			[comment CLOSE traverse all the DataProcessings/]
[for (d : DataProcessing | aWorkflow.dataprocessing)]							[comment OPEN traverse all the DataProcessings/]
	[if (d.incoming = null and d.outgoing<>null)]								[comment OPEN checks if the DataProcessing is the first (no input link, an output link)/]
		[if (d.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
			[let dpd : Library::DataProcessingDefinition = d.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
				[if (dpd.oclIsKindOf(Library::Transformation))]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[callContractsRecursiveTemplate(d)/]
					[/let]
				[elseif (dpd.oclIsKindOf(Library::Job))]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | d._in)]
							[if (arg.oclIsKindOf(DataDictionary))]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_in.fileName/]'
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_in.columnDelimiter/]'
								[/let]
							[/if]
						[/for]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | d.out)]
							[if (arg.oclIsKindOf(DataDictionary))]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_out.fileName/]'
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_out.columnDelimiter/]'
								[/let]
							[/if]
						[/for]


						[for (dataP : DataProcessing | libJ.workflow.dataprocessing)]
							[if (dataP.incoming = null and dataP.outgoing=null)]
								[for (c : Contract | dataP.contract)]										[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::PRECONDITION)]					[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Precondition/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
								[for (c : Contract | dataP.contract)]										[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::POSTCONDITION)]					[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Postcondition/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
								[for (c : Contract | dataP.contract)]										[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::INVARIANT)]						[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
									[/if]																[comment CLOSE checks if the if the contract is a Invariant/]
								[/for]																	[comment CLOSE traverse all the contracts in the DataProcessing/]
							[/if]
						[/for]
						[for (dataP : DataProcessing | libJ.workflow.dataprocessing)]
							[if (dataP.incoming = null and dataP.outgoing<>null)]
[callContractsModifiedRecursiveTemplate(dataP, d)/]
							[/if]
						[/for]
					[/let]
				[/if]
			[/let]
		[/if]
	[/if]																		[comment CLOSE checks if the DataProcessing is the first (no input link, an output link)/]
[/for]																			[comment CLOSE traverse all the DataProcessings/]
set_logger("contracts")
generateWorkflow()
[/file]
[/template]


[template public callContractsModifiedRecursiveTemplate(dw : DataProcessing, d_or : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (dw.incoming <> null)]														[comment OPEN checks if there is a previous DataProcessing/]
	[if (dw.incoming.source.dataProcessingDefinition.name<>'split')]
	[dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/], sep=[dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_sep)

	[else]
		[for (dc : DataDictionary | dw.inputPort)]								[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | dw.incoming.source.outputPort)]		[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName = dc_out.fileName)]									[comment OPEN checks that both datasets are the same/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dc.name.replaceAll('[(),\\s]+', '_')/], sep=[dc.name.replaceAll('[(),\\s]+', '_')/]_sep)

				[/if]															[comment CLOSE checks that both datasets are the same/]
			[/for]																[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																	[comment CLOSE read all the input datasets and store them/]
	[/if]
[else]																			[comment OPEN checks that there is not a previous DataProcessing/]
	[for (dc : DataDictionary | dw.inputPort)]									[comment OPEN read all the input datasets and store them/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dc.name.replaceAll('[(),\\s]+', '_')/], sep = [dc.name.replaceAll('[(),\\s]+', '_')/]_sep)
	[/for]																		[comment CLOSE read all the input datasets and store them/]
[/if]																			[comment CLOSE checks if there is a previous DataProcessingor not/]
[for (dc : DataDictionary | dw.outputPort)]										[comment OPEN read all the output datasets and store them/]
	if os.fileName.exists([dc.name.replaceAll('[(),\\s]+', '_')/]):		#If the output DataDictionary exists, we store it
		[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dc.name.replaceAll('[(),\\s]+', '_')/], sep = [dc.name.replaceAll('[(),\\s]+', '_')/]_sep)

[/for]																			[comment CLOSE read all the output datasets and store them/]
[for (c : Contract | dw.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::PRECONDITION)]							[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Precondition/]
[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | dw.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::POSTCONDITION)]							[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Postcondition/]
[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | dw.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::INVARIANT)]								[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Invariant/]
[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
[if (dw.outgoing<>null)]															[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = dw.outgoing.target.oclAsType(DataProcessing)] [comment OPEN assigns the next DataProcessing to a variable/]
[callContractsModifiedRecursiveTemplate(nextDp, d_or)/]										[comment Recursive call to this template to generate all the DataProcessings linked to the first/]
	[/let]																		[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																			[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public callContractsRecursiveTemplate(d : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (d.incoming <> null)]														[comment OPEN checks if there is a previous DataProcessing/]
	[if (d.incoming.source.dataProcessingDefinition.name<>'split')]
	[d.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[d.inputPort->first().fileName/]', sep='[d.inputPort->first().columnDelimiter/]')

	[else]
		[for (dc : DataDictionary | d.inputPort)]								[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | d.incoming.source.outputPort)]		[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName = dc_out.fileName)]									[comment OPEN checks that both datasets are the same/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[dc_out.fileName/]', sep='[dc_out.columnDelimiter/]')

				[/if]															[comment CLOSE checks that both datasets are the same/]
			[/for]																[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																	[comment CLOSE read all the input datasets and store them/]
	[/if]
[else]																			[comment OPEN checks that there is not a previous DataProcessing/]
	[for (dc : DataDictionary | d.inputPort)]									[comment OPEN read all the input datasets and store them/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[dc.fileName/]', sep = '[dc.columnDelimiter/]')
	[/for]																		[comment CLOSE read all the input datasets and store them/]
[/if]																			[comment CLOSE checks if there is a previous DataProcessingor not/]
[for (dc : DataDictionary | d.outputPort)]										[comment OPEN read all the output datasets and store them/]
	if os.fileName.exists('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[dc.fileName/]'):		#If the output DataDictionary exists, we store it
		[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[dc.fileName/]', sep = '[dc.columnDelimiter/]')

[/for]																			[comment CLOSE read all the output datasets and store them/]
[for (c : Contract | d.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::PRECONDITION)]							[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Precondition/]
[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | d.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::POSTCONDITION)]							[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Postcondition/]
[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | d.contract)]												[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::INVARIANT)]								[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
	[/if]																		[comment CLOSE checks if the if the contract is a Invariant/]
[/for]																			[comment CLOSE traverse all the contracts in the DataProcessing/]
[if (d.outgoing<>null)]															[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = d.outgoing.target.oclAsType(DataProcessing)] [comment OPEN assigns the next DataProcessing to a variable/]
[callContractsRecursiveTemplate(nextDp)/]										[comment Recursive call to this template to generate all the DataProcessings linked to the first/]
	[/let]																		[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																			[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public generateTransformationsDataProcessing(aWorkflow : Workflow)]
[file ('transformations_Job_'+aWorkflow.name.replaceAll('[(),\\s]+', '_')+'.py', false, 'UTF-8')]
import pandas as pd
import numpy as np
import functions.data_transformations as data_transformations
from helpers.enumerations import Belong, Operator, Operation, SpecialType, DataType, DerivedType, Closure, FilterType
from helpers.logger import set_logger

[comment generate the imports and the class/]
def generateWorkflow():
[for (d : DataProcessing | aWorkflow.dataprocessing)]							[comment OPEN traverse all the DataProcessings/]
	[if (d.incoming = null and d.outgoing=null)]								[comment OPEN checks if the DataProcessing is isolated (no links)/]
	#-----------------New DataProcessing-----------------
		[if (d.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
			[let dpd : Library::DataProcessingDefinition = d.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
				[if (dpd.oclIsKindOf(Library::Transformation))]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
						[for (dc : DataDictionary | d.inputPort)]									[comment OPEN read all the input datasets and store them/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dc.fileName/]', sep = '[dc.columnDelimiter/]')

						[/for]																		[comment CLOSE read all the input datasets and store them/]
[generateCallTransformation(d, d)/]
					[/let]
				[elseif (dpd.oclIsKindOf(Library::Job))]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | d._in)]
							[if (arg.oclIsKindOf(DataDictionary))]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_in.fileName/]'
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_in.columnDelimiter/]'
								[/let]
							[/if]
						[/for]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | d.out)]
							[if (arg.oclIsKindOf(DataDictionary))]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_out.fileName/]'
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_out.columnDelimiter/]'
								[/let]
							[/if]
						[/for]
						[for (dp : DataProcessing | libJ.workflow.dataprocessing)]
							[if (dp.incoming = null and dp.outgoing=null)]
[generateCallTransformation(dp, d)/]
							[/if]
						[/for]
						[for (dp : DataProcessing | libJ.workflow.dataprocessing)]
							[if (dp.incoming = null and dp.outgoing<>null)]
[callTransformationsModifiedRecursiveTemplate(dp, d)/]
							[/if]
						[/for]
					[/let]
				[/if]
			[/let]
		[/if]
	[/if]																		[comment CLOSE checks if the DataProcessing is isolated (no links)/]
[/for]																			[comment CLOSE traverse all the DataProcessings/]
[for (d : DataProcessing | aWorkflow.dataprocessing)]							[comment OPEN traverse all the DataProcessings/]
	[if (d.incoming = null and d.outgoing<>null)]								[comment OPEN checks if the DataProcessing is the first (no input link, an output link)/]
		[if (d.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
			[let dpd : Library::DataProcessingDefinition = d.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
				[if (dpd.oclIsKindOf(Library::Transformation))]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[callTransformationsRecursiveTemplate(d)/]
					[/let]
				[elseif (dpd.oclIsKindOf(Library::Job))]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | d._in)]
							[if (arg.oclIsKindOf(DataDictionary))]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_in.fileName/]'
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_in.columnDelimiter/]'
								[/let]
							[/if]
						[/for]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | d.out)]
							[if (arg.oclIsKindOf(DataDictionary))]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[dd_out.fileName/]'
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]_sep='[dd_out.columnDelimiter/]'
								[/let]
							[/if]
						[/for]
						[for (dp : DataProcessing | libJ.workflow.dataprocessing)]
							[if (dp.incoming = null and dp.outgoing=null)]
[generateCallTransformation(dp, d)/]
							[/if]
						[/for]
						[for (dp : DataProcessing | libJ.workflow.dataprocessing)]
							[if (dp.incoming = null and dp.outgoing<>null)]
[callTransformationsModifiedRecursiveTemplate(dp, d)/]
							[/if]
						[/for]
					[/let]
				[/if]
			[/let]
		[/if]
	[/if]																		[comment CLOSE checks if the DataProcessing is the first (no input link, an output link)/]
[/for]																			[comment CLOSE traverse all the DataProcessings/]
set_logger("transformations")
generateWorkflow()
[/file]
[/template]


[template public callTransformationsModifiedRecursiveTemplate(dw : DataProcessing, d_or : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (dw.incoming <> null)]														[comment OPEN checks if there is an incoming link/]
	[if (dw.incoming.source.dataProcessingDefinition.name<>'split')]
	[dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/], sep=[dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)

	[else]
		[for (dc : DataDictionary | dw.inputPort)]								[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | dw.incoming.source.outputPort)]		[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName = dc_out.fileName)]									[comment OPEN checks that both datasets are the same/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dc.name.replaceAll('[(),\\s]+', '_')/], sep=[dc.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)

				[/if]															[comment CLOSE checks that both datasets are the same/]
			[/for]																[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																	[comment CLOSE read all the input datasets and store them/]
	[/if]
[else]																			[comment OPEN checks if there is not an incoming link/]
	[for (dc : DataDictionary | dw.inputPort)]									[comment OPEN read all the input datasets and store them/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dc.name.replaceAll('[(),\\s]+', '_')/], sep = [dc.name.replaceAll('[(),\\s]+', '_')/]_sep)
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dc.name.replaceAll('[(),\\s]+', '_')/])
	[/for]																		[comment CLOSE read all the input datasets and store them/]
[/if]																			[comment CLOSE checks if there is an incoming link or not/]
	[generateCallTransformation(dw, d_or)/]
[if (dw.outgoing<>null)]															[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = dw.outgoing.target.oclAsType(DataProcessing)] [comment OPEN assigns the next DataProcessing to a variable/]
[callTransformationsModifiedRecursiveTemplate(nextDp, d_or)/]
	[/let]																		[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																			[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public callTransformationsRecursiveTemplate(d : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (d.incoming <> null)]														[comment OPEN checks if there is an incoming link/]
	[if (d.incoming.source.dataProcessingDefinition.name<>'split')]
	[d.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[d.inputPort->first().fileName/]', sep='[d.inputPort->first().columnDelimiter/]', index_col=0)

	[else]
		[for (dc : DataDictionary | d.inputPort)]								[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | d.incoming.source.outputPort)]		[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName = dc_out.fileName)]									[comment OPEN checks that both datasets are the same/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[dc_out.fileName/]', sep='[dc_out.columnDelimiter/]', index_col=0)

				[/if]															[comment CLOSE checks that both datasets are the same/]
			[/for]																[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																	[comment CLOSE read all the input datasets and store them/]
	[/if]
[else]																			[comment OPEN checks if there is not an incoming link/]
	[for (dc : DataDictionary | d.inputPort)]									[comment OPEN read all the input datasets and store them/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[dc.fileName/]', sep = '[dc.columnDelimiter/]')
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[dc.fileName/]')
	[/for]																		[comment CLOSE read all the input datasets and store them/]
[/if]																			[comment CLOSE checks if there is an incoming link or not/]
	[generateCallTransformation(d, d)/]
[if (d.outgoing<>null)]															[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = d.outgoing.target.oclAsType(DataProcessing)] [comment OPEN assigns the next DataProcessing to a variable/]
[callTransformationsRecursiveTemplate(nextDp)/]
	[/let]																		[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																			[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public generateCallTransformation(dataProcessing : DataProcessing, dw_modified : DataProcessing)]
[comment]The code below generates the mapping transformations[/comment]
[if (dataProcessing.parameter->notEmpty())]																[comment OPEN checks that there are parameters in the DataProcessing/]
[if (dataProcessing.dataProcessingDefinition.name='mapping')]											[comment OPEN checks that the DataProcessing is a Mapping (FixValue-FixValue)/]
[for (dd_out : DataDictionary | dataProcessing.outputPort)]												[comment OPEN traverse all the output DataDictionaries (in theory there is only 1)/]
	[for (dd_in : DataDictionary | dataProcessing.inputPort)]											[comment OPEN traverse all the input DataDictionaries (in theory there is only 1)/]
		[if dd_in.datafield->notEmpty()]																[comment OPEN checks that there is at least one DataField/]
			[for (df : DataField | dd_in.datafield)]													[comment OPEN traverse all the DataFields in the input DataDictionary (the same as in the output DataDictionary)/]
input_values_list=['['/][for (aParameter : Parameter | dataProcessing.parameter)  separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)][if (df.dataType=DataType::String or df.dataType=DataType::DateTime or df.dataType=DataType::Time)]'[map.inValue/]'[else][map.inValue/][/if][/let][/if][/for][']'/]
output_values_list=['['/][for (aParameter : Parameter | dataProcessing.parameter)  separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)][if (df.dataType=DataType::String or df.dataType=DataType::DateTime or df.dataType=DataType::Time)]'[map.outvalue/]'[else][map.outvalue/][/if][/let][/if][/for][']'/]
data_type_input_list=['['/][for (aParameter : Parameter | dataProcessing.parameter) separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)][if (df.dataType=DataType::String)]DataType(0)[elseif (df.dataType=DataType::Time)]DataType(1)[elseif (df.dataType=DataType::Integer)]DataType(2)[elseif (df.dataType=DataType::DateTime)]DataType(3)[elseif (df.dataType=DataType::Boolean)]DataType(4)[elseif (df.dataType=DataType::Double)]DataType(5)[elseif (df.dataType=DataType::Float)]DataType(6)[else]None[/if][/let][/if][/for][']'/]
data_type_output_list=['['/][for (aParameter : Parameter | dataProcessing.parameter) separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)][if (df.dataType=DataType::String)]DataType(0)[elseif (df.dataType=DataType::Time)]DataType(1)[elseif (df.dataType=DataType::Integer)]DataType(2)[elseif (df.dataType=DataType::DateTime)]DataType(3)[elseif (df.dataType=DataType::Boolean)]DataType(4)[elseif (df.dataType=DataType::Double)]DataType(5)[elseif (df.dataType=DataType::Float)]DataType(6)[else]None[/if][/let][/if][/for][']'/]


[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=data_transformations.transform_fix_value_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_df, input_values_list=input_values_list,
															  output_values_list=output_values_list,
						                                      data_type_input_list = data_type_input_list,
						                                      data_type_output_list = data_type_output_list, field_in = '[df.displayName/]', field_out = '[df.out.displayName/]')

				[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
					[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
						[if (dpd.oclIsKindOf(Library::Transformation))]
							[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[dd_out.fileName/]')	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[dd_out.fileName/]', sep='[dd_out.columnDelimiter/]', index_col=0)
							[/let]
						[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/], sep=[dd_out.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)
						[/if]
					[/let]
				[/if]
			[/for]																						[comment CLOSE traverse all the DataFields in the input DataDictionary (the same as in the output DataDictionary)/]
		[else]																							[comment OPEN checks that there are no DataField/]
input_values_list=['['/][for (aParameter : Parameter | dataProcessing.parameter)  separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)]'[map.inValue/]'[/let][/if][/for][']'/]
output_values_list=['['/][for (aParameter : Parameter | dataProcessing.parameter)  separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)]'[map.outvalue/]'[/let][/if][/for][']'/]

[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=data_transformations.transform_fix_value_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_df, input_values_list=input_values_list,
															  output_values_list=output_values_list,
						                                      data_type_input_list = None,
						                                      data_type_output_list = None, field_in = None, filed_out = None)
			[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
				[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
					[if (dpd.oclIsKindOf(Library::Transformation))]
							[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[dd_out.fileName/]')	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[dd_out.fileName/]', sep='[dd_out.columnDelimiter/]', index_col=0)
							[/let]
					[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/], sep=[dd_out.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)
					[/if]
				[/let]
			[/if]			
		[/if]																							[comment CLOSE checks that there are DataFields or not/]
	[/for]																								[comment CLOSE traverse all the input DataDictionaries (in theory there is only 1)/]
[/for]																									[comment CLOSE traverse all the output DataDictionaries (in theory there is only 1)/]
[/if]																									[comment CLOSE checks that the DataProcessing is a Mapping (FixValue-FixValue)/]
[comment]The code below generates the transformations different from mapping for every field[/comment]
[if (dataProcessing.dataProcessingDefinition.name<>'mapping')]											[comment OPEN checks that the DataProcessing is not a Mapping/]
[for (dd_in : DataDictionary | dataProcessing.inputPort)]												[comment OPEN traverse all the input DataDictionaries/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_df.copy()
[/for]																									[comment CLOSE traverse all the input DataDictionaries/]
[for (p : Parameter | dataProcessing.parameter)]														[comment OPEN traverse all the parameters in the DataProcessing/]
    [if (p.oclIsKindOf(ImputeType))]																	[comment OPEN checks that the parameter is an ImputeType/]
        [let imType : ImputeType = p.oclAsType(ImputeType)]												[comment OPEN assigns the parameter to be an ImputeType/]
			[for (dd_out : DataDictionary | dataProcessing.outputPort)]									[comment OPEN traverse all the output DataDictionaries (in theory there is only 1)/]
				[for (dd_in : DataDictionary | dataProcessing.inputPort)]								[comment OPEN traverse all the input DataDictionaries (in theory there is only 1)/]
					[if dd_in.datafield->notEmpty()]													[comment OPEN checks that there is at least DataField/]
						[for (df : DataField | dd_in.datafield)]										[comment OPEN traverse all the DataFields in the input DataDictionary (the same as in the output DataDictionary)/]
missing_values_list=['['/][for (mv : ValueField | df.missingValues) separator(', ')][if (df.dataType=DataType::String or df.dataType=DataType::DateTime or df.dataType=DataType::Time)]'[mv.value/]'[else][mv.value/][/if][/for][']'/]

							[if (dataProcessing.dataProcessingDefinition.name='imputeByFixValue')]		[comment OPEN checks that the DataProcessing is an imputeByFixValue(SpecialValue_FixValue)/]
				            	[if (imType.oclIsKindOf(FixValue))]										[comment OPEN checks that the ImputeType is a FixValue/]
            	    				[let fv : FixValue = imType.oclAsType(FixValue)]					[comment OPEN assigns the ImputeType as a FixValue/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_special_value_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
															  special_type_input=SpecialType([if (fv.imputeValue=SpecialValue::Missing)]0[elseif (fv.imputeValue=SpecialValue::Invalid)]1[elseif (fv.imputeValue=SpecialValue::Outlier)]2[/if]), fix_value_output=[if (df.dataType=DataType::String or df.dataType=DataType::DateTime or df.dataType=DataType::Time)]'[fv.value/]'[else][fv.value/][/if],
															  missing_values=missing_values_list,		
						                                      data_type_output = [if (df.dataType=DataType::String)]DataType(0)[elseif (df.dataType=DataType::Time)]DataType(1)[elseif (df.dataType=DataType::Integer)]DataType(2)[elseif (df.dataType=DataType::DateTime)]DataType(3)[elseif (df.dataType=DataType::Boolean)]DataType(4)[elseif (df.dataType=DataType::Double)]DataType(5)[elseif (df.dataType=DataType::Float)]DataType(6)[else]None[/if],
															  axis_param=0, field_in = '[df.displayName/]', field_out = '[df.out.displayName/]')

			                		[/let]																[comment CLOSE assigns the ImputeType as a FixValue/]
			            		[/if]																	[comment CLOSE checks that the ImputeType is a FixValue/]
							[/if]																		[comment CLOSE checks that the DataProcessing is an imputeByFixValue(SpecialValue_FixValue)/]
							[if (dataProcessing.dataProcessingDefinition.name='imputeByDerivedValue')]	[comment OPEN checks that the DataProcessing is an imputeByDerivedValue(SpecialValue_DerivedValue)/]
	        					[if (imType.oclIsKindOf(DerivedValue))]									[comment OPEN checks that the ImputeType is a DerivedValue/]
                					[let dv : DerivedValue = imType.oclAsType(DerivedValue)]			[comment OPEN assigns the ImputeType as a DerivedValue/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_special_value_derived_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
															  special_type_input=SpecialType([if (dv.imputeValue=SpecialValue::Missing)]0[elseif (dv.imputeValue=SpecialValue::Invalid)]1[elseif (dv.imputeValue=SpecialValue::Outlier)]2[/if]), derived_type_output=DerivedType([if (dv.type=DerivedType::MostFrequent)]0[elseif (dv.type=DerivedType::Previous)]1[elseif (dv.type=DerivedType::Next)]2[/if]),
															  missing_values=missing_values_list,		
															  axis_param=0, field_in = '[df.displayName/]', field_out = '[df.out.displayName/]')

			                		[/let]																[comment CLOSE assigns the ImputeType as a DerivedValue/]
			            		[/if]																	[comment CLOSE checks that the ImputeType is a DerivedValue/]
							[/if]																		[comment CLOSE checks that the DataProcessing is an imputeByDerivedValue(SpecialValue_DerivedValue)/]
							[if (dataProcessing.dataProcessingDefinition.name='imputeByNumericOp')]		[comment OPEN checks that the DataProcessing is an imputeByNumericOp(SpecialValue_NumOp)/]
            					[if (imType.oclIsKindOf(NumOp))]										[comment OPEN checks that the ImputeType is a NumOp/]
                					[let nop : NumOp = imType.oclAsType(NumOp)]							[comment OPEN assigns the ImputeType as a NumOp/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_special_value_num_op(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
															  special_type_input=SpecialType([if (nop.imputeValue=SpecialValue::Missing)]0[elseif (nop.imputeValue=SpecialValue::Invalid)]1[elseif (nop.imputeValue=SpecialValue::Outlier)]2[/if]), num_op_output=Operation([if (nop.operation=Operation::Interpolation)]0[elseif (nop.operation=Operation::Mean)]1[elseif (nop.operation=Operation::Median)]2[elseif (nop.operation=Operation::Closest)]3[/if]),
															  missing_values=missing_values_list,		
															  axis_param=0, field_in = '[df.displayName/]', field_out = '[df.out.displayName/]')

				                	[/let]																[comment CLOSE assigns the ImputeType as a NumOp/]
				            	[/if]																	[comment CLOSE checks that the ImputeType is a NumOp/]
							[/if]																		[comment CLOSE checks that the DataProcessing is an imputeByNumericOp(SpecialValue_NumOp)/]
						[/for]																			[comment CLOSE traverse all the DataFields in the input DataDictionary (the same as in the output DataDictionary)/]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
						[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
							[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
								[if (dpd.oclIsKindOf(Library::Transformation))]
									[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[dd_out.fileName/]')	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[dd_out.fileName/]', sep='[dd_out.columnDelimiter/]', index_col=0)
									[/let]
								[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/], sep=[dd_out.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)
								[/if]
							[/let]
						[/if]
[comment]The code below generates the same transformation generated above but for the whole dataframe instead of a field[/comment]
					[else]																				[comment OPEN checks that there are no DataFields/]
						[if (dataProcessing.dataProcessingDefinition.name='imputeByFixValue')]			[comment OPEN checks that the DataProcessing is an imputeByFixValue(SpecialValue_FixValue)/]
							[if (imType.oclIsKindOf(FixValue))]											[comment OPEN checks that the ImputeType is a FixValue/]
			            	[let fv : FixValue = imType.oclAsType(FixValue)]							[comment OPEN assigns the ImputeType as a FixValue/]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=data_transformations.transform_special_value_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_df,
															  special_type_input=SpecialType([if (imType.imputeValue=SpecialValue::Missing)]0[elseif (imType.imputeValue=SpecialValue::Invalid)]1[elseif (imType.imputeValue=SpecialValue::Outlier)]2[/if]), fix_value_output='[fv.value/]', missing_values=None
						                                      data_type_output = None, axis_param=0, field_in = None, field_out = None)		

			            		[/let]																	[comment CLOSE assigns the ImputeType as a FixValue/]
			        		[/if]																		[comment CLOSE checks that the ImputeType is a FixValue/]
						[/if]																			[comment CLOSE checks that the DataProcessing is an imputeByFixValue(SpecialValue_FixValue)/]
						[if (dataProcessing.dataProcessingDefinition.name='imputeByDerivedValue')]		[comment OPEN checks that the DataProcessing is an imputeByDerivedValue(SpecialValue_DerivedValue)/]	
	        				[if (imType.oclIsKindOf(DerivedValue))]										[comment OPEN checks that the ImputeType is a DerivedValue/]
	            				[let dv : DerivedValue = imType.oclAsType(DerivedValue)]				[comment OPEN assigns the ImputeType as a DerivedValue/]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=data_transformations.transform_special_value_derived_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_df,
															  special_type_input=SpecialType([if (dv.imputeValue=SpecialValue::Missing)]0[elseif (dv.imputeValue=SpecialValue::Invalid)]1[elseif (dv.imputeValue=SpecialValue::Outlier)]2[/if]), derived_type_output=DerivedType([if (dv.type=DerivedType::MostFrequent)]0[elseif (dv.type=DerivedType::Previous)]1[elseif (dv.type=DerivedType::Next)]2[/if]),
															  missing_values=None, axis_param=0, field_in = None, field_out = None)		

				            	[/let]																	[comment CLOSE assigns the ImputeType as a DerivedValue/]
				        	[/if]																		[comment CLOSE checks that the ImputeType is a DerivedValue/]
						[/if]																			[comment CLOSE checks that the DataProcessing is an imputeByDerivedValue(SpecialValue_DerivedValue)/]
						[if (dataProcessing.dataProcessingDefinition.name='imputeByNumericOp')]			[comment OPEN checks that the DataProcessing is an imputeByNumericOp(SpecialValue_NumOp)/]
        					[if (imType.oclIsKindOf(NumOp))]											[comment OPEN checks that the ImputeType is a NumOp/]
                				[let nop : NumOp = imType.oclAsType(NumOp)]								[comment OPEN assigns the ImputeType as a NumOp/]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=data_transformations.transform_special_value_num_op(data_dictionary=[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df,
															  special_type_input=SpecialType([if (nop.imputeValue=SpecialValue::Missing)]0[elseif (nop.imputeValue=SpecialValue::Invalid)]1[elseif (nop.imputeValue=SpecialValue::Outlier)]2[/if]), num_op_output=Operation([if (nop.operation=Operation::Interpolation)]0[elseif (nop.operation=Operation::Mean)]1[elseif (nop.operation=Operation::Median)]2[elseif (nop.operation=Operation::Closest)]3[/if]),
															  missing_values=None, axis_param=0, field_in = None, field_out = None)		

				            	[/let]																	[comment CLOSE assigns the ImputeType as a NumOp/]
				        	[/if]																		[comment CLOSE checks that the ImputeType is a NumOp/]
						[/if]																			[comment CLOSE checks that the DataProcessing is an imputeByNumericOp(SpecialValue_NumOp)/]
						[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
							[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
								[if (dpd.oclIsKindOf(Library::Transformation))]
									[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[dd_out.fileName/]')	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[dd_out.fileName/]', sep='[dd_out.columnDelimiter/]', index_col=0)
									[/let]
								[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/], sep=[dd_out.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)
								[/if]
							[/let]
						[/if]
					[/if]																				[comment CLOSE checks if there are DataFields or not/]
				[/for]																					[comment CLOSE traverse all the input DataDictionaries (in theory there is only 1)/]
			[/for]																						[comment CLOSE traverse all the output DataDictionaries (in theory there is only 1)/]
		[/let]																							[comment CLOSE assigns the parameter to be an ImputeType/]
	[/if]																								[comment CLOSE checks that the parameter is an ImputeType/]
	[if (dataProcessing.dataProcessingDefinition.name='binner')]										[comment OPEN checks that the DataProcessing is a Binner (interval_FixValue)/]
		[if (p.oclIsKindOf(DiscretizeBin))]																[comment OPEN checks that the parameter is an DiscretizeBin/]
			[let binner : DiscretizeBin = p.oclAsType(DiscretizeBin)]									[comment OPEN assigns the parameter to be a DiscretizeBin/]
				[for (dd_out : DataDictionary | dataProcessing.outputPort)]								[comment OPEN traverse all the output DataDictionaries (in theory there is only 1)/]
					[for (dd_in : DataDictionary | dataProcessing.inputPort)]							[comment OPEN traverse all the input DataDictionaries (in theory there is only 1)/]
						[if dd_in.datafield->notEmpty()]												[comment OPEN checks that there is at least DataField/]
							[for (df : DataField | dd_in.datafield)]									[comment OPEN traverse all the DataFields in the input DataDictionary/]
								[for (interval : Interval | binner.interval)]							[comment OPEN traverse all the intervals in the binner/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_interval_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
															  left_margin=[interval.leftMargin/], right_margin=[interval.rightMargin/],
															  closure_type=Closure([if (interval.clousure=ClosureType::opeOpen)]0[elseif(interval.clousure=ClosureType::openClosed)]1[elseif(interval.clousure=ClosureType::closedOpen)]2[elseif(interval.clousure=ClosureType::closedClosed)]3[/if]),
															  fix_value_output=[if (df.out.dataType=DataType::String or df.out.dataType=DataType::DateTime or df.out.dataType=DataType::Time)]'[binner.binValue/]'[else][binner.binValue/][/if],
						                                      data_type_output = [if (df.out.dataType=DataType::String)]DataType(0)[elseif (df.out.dataType=DataType::Time)]DataType(1)[elseif (df.out.dataType=DataType::Integer)]DataType(2)[elseif (df.out.dataType=DataType::DateTime)]DataType(3)[elseif (df.out.dataType=DataType::Boolean)]DataType(4)[elseif (df.out.dataType=DataType::Double)]DataType(5)[elseif (df.out.dataType=DataType::Float)]DataType(6)[else]None[/if],
															  field_in = '[df.displayName/]',
															  field_out = '[df.out.displayName/]')

								[/for]																	[comment CLOSE traverse all the intervals in the binner/]
							[/for]																		[comment CLOSE traverse all the DataFields in the input DataDictionary/]
						[else]																			[comment OPEN checks that there are not DataFields/]
							[for (interval : Interval | binner.interval)]								[comment OPEN traverse all the intervals in the binner/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_interval_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
														  left_margin=[interval.leftMargin/], right_margin=[interval.rightMargin/],
														  closure_type=Closure([if (interval.clousure=ClosureType::opeOpen)]0[elseif(interval.clousure=ClosureType::openClosed)]1[elseif(interval.clousure=ClosureType::closedOpen)]2[elseif(interval.clousure=ClosureType::closedClosed)]3[/if]),
														  fix_value_output='[binner.binValue/]', data_type_output = None, field_in = None, field_out = None)

							[/for]																		[comment CLOSE traverse all the intervals in the binner/]
						[/if]																			[comment CLOSE checks if there are DataFields/]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
						[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
							[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
								[if (dpd.oclIsKindOf(Library::Transformation))]
									[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[dd_out.fileName/]')	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[dd_out.fileName/]', sep='[dd_out.columnDelimiter/]', index_col=0)
									[/let]
								[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/], sep=[dd_out.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)
								[/if]
							[/let]
						[/if]
					[/for]																				[comment CLOSE traverse all the intervals in the binner/]
				[/for]																					[comment CLOSE traverse all the DataFields in the input DataDictionary/]
			[/let]																						[comment OPEN assigns the parameter to be an DiscretizeBin/]
		[/if]																							[comment CLOSE checks that the parameter is an DiscretizeBin/]
	[/if]																								[comment CLOSE checks that the DataProcessing is a Binner (interval_FixValue)/]
	[if (p.oclIsKindOf(DerivedField))]																	[comment OPEN checks that the parameter is a DerivedField/]
		[let derf : DerivedField = p.oclAsType(DerivedField)]											[comment OPEN assigns the parameter to be a DerivedField/]
			[for (dd_in : DataDictionary | dataProcessing.inputPort)]									[comment OPEN traverse al the input DataDictionaries/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_derived_field(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
															  data_type_output = [if (derf.der_dataField.dataType=DataType::String)]DataType(0)[elseif (derf.der_dataField.dataType=DataType::Time)]DataType(1)[elseif (derf.der_dataField.dataType=DataType::Integer)]DataType(2)[elseif (derf.der_dataField.dataType=DataType::DateTime)]DataType(3)[elseif (derf.der_dataField.dataType=DataType::Boolean)]DataType(4)[elseif (derf.der_dataField.dataType=DataType::Double)]DataType(5)[elseif (derf.der_dataField.dataType=DataType::Float)]DataType(6)[else]None[/if],
															  field_in = '[derf.der_dataField._in.displayName/]', field_out = '[derf.der_dataField.displayName/]')

				[for (dd_out : DataDictionary | dataProcessing.outputPort)]								[comment OPEN traverse al the output DataDictionaries/]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
					[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
						[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
							[if (dpd.oclIsKindOf(Library::Transformation))]
								[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[dd_out.fileName/]')	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[dd_out.fileName/]', sep='[dd_out.columnDelimiter/]', index_col=0)
								[/let]
							[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/], sep=[dd_out.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)
							[/if]
						[/let]
					[/if]
				[/for]																					[comment CLOSE traverse all the output DataDictionaries/]
			[/for]																						[comment CLOSE traverse all the input DataDictionaries/]
		[/let]																							[comment CLOSE assigns the parameter to be a DerivedField/]
	[/if]																								[comment CLOSE checks that the parameter is a DerivedField/]
	[if (dataProcessing.dataProcessingDefinition.name='categoricalToContinuous')]						[comment OPEN checks that the DataProcessing is categoricalToContinuous(StringToNumber)/]
		[if (p.oclIsKindOf(CastType))]																	[comment OPEN checks that the parameter is a CastType/]
			[let cast : CastType = p.oclAsType(CastType)]												[comment OPEN assigns the variable to be a CastType/]
				[for (dd_in : DataDictionary | dataProcessing.inputPort)]								[comment OPEN traverse the input DataDictionaries/]
					[for (dd_out : DataDictionary | dataProcessing.outputPort)]							[comment OPEN traverse the output DataDictionaries/]
						[for (df : DataField | dd_in.datafield)]										[comment OPEN traverse the DataFields/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_cast_type(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
																data_type_output= [if (cast.type=DataType::String)]DataType(0)[elseif (cast.type=DataType::Time)]DataType(1)[elseif (cast.type=DataType::Integer)]DataType(2)[elseif (cast.type=DataType::DateTime)]DataType(3)[elseif (cast.type=DataType::Boolean)]DataType(4)[elseif (cast.type=DataType::Double)]DataType(5)[elseif (cast.type=DataType::Float)]DataType(6)[else]None[/if],
																field='[df.displayName/]')

						[/for]																			[comment CLOSE traverse the DataFields/]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
						[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
							[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
								[if (dpd.oclIsKindOf(Library::Transformation))]
									[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[dd_out.fileName/]')	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[dd_out.fileName/]', sep='[dd_out.columnDelimiter/]', index_col=0)
									[/let]
								[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/], sep=[dd_out.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)
								[/if]
							[/let]
						[/if]
					[/for]																				[comment CLOSE traverse the output DataDictionaries/]
				[/for]																					[comment CLOSE traverse the input DataDictionaries/]
			[/let]																						[comment CLOSE assigns the variable to be a CastType/]
		[/if]																							[comment CLOSE checks that the parameter is a CastType/]
	[/if]																								[comment CLOSE checks that the DataProcessing is categoricalToContinuous(StringToNumber)/]
	[if (dataProcessing.dataProcessingDefinition.name='rowFilter' or dataProcessing.dataProcessingDefinition.name='rowFilterRange')]		[comment OPEN checks that the DataProcessing is rowFilter/]
		[if (p.oclIsKindOf(FilterValue))]																[comment OPEN checks that the parameter is a FilterValue/]
			[for (dd_in : DataDictionary | dataProcessing.inputPort)]
				[let fValue : FilterValue = p.oclAsType(FilterValue)]									[comment OPEN assigns the variable to be a FilterValue/]
columns_[fValue.filterValueDef.name/]=['['/][for (df : DataField | dd_in.datafield) separator (', ')]'[df.displayName/]'[/for][']'/]

					[if (not fValue.primitive->isEmpty())]												[comment OPEN checks that the list of primitives is not empty/]
filter_fix_value_list_[fValue.filterValueDef.name/]=['['/][for (prim : Primitive | fValue.primitive) separator (', ')][prim.value/][/for][']'/]

						[for (dd_out : DataDictionary | dataProcessing.outputPort)]						[comment OPEN traverse all the output DataDictionaries/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_filter_rows_primitive(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
																										columns=columns_[fValue.filterValueDef.name/],
																	                                    filter_fix_value_list=filter_fix_value_list_[fValue.filterValueDef.name/],
																										filter_type=FilterType([if (fValue.filterType=FilterType::EXCLUDE)]0[else]1[/if]))
						[/for]																			[comment CLOSE traverse all the output DataDictionaries/]
					[/if]																				[comment CLOSE checks that the list of primitives is not empty/]
					[if (not fValue.matchingvalue->isEmpty())]											[comment OPEN checks that the list of MatchingValues is not empty/]
						[let filteredSet : OrderedSet(SpecialValues) = fValue.matchingvalue->select(e | e.oclIsKindOf(SpecialValues))]	[comment OPEN assigns to a set only the SpecialValues/]
							[if (not filteredSet->isEmpty())]											[comment OPEN checks that list of SpecialValues is not empty/]
dicc_[fValue.filterValueDef.name/]={[for (df : DataField | dd_in.datafield) separator (', ')]'[df.displayName/]':{[for (spVal : SpecialValues | filteredSet) separator (', ')][if (spVal.specialType=SpecialValue::Missing)]'missing': ['['/][for (mv : ValueField | df.missingValues) separator(', ')][if (df.dataType=DataType::String or df.dataType=DataType::Time or df.dataType=DataType::DateTime)]'[mv.value/]'[else][mv.value/][/if][/for][']'/][elseif(spVal.specialType=SpecialValue::Invalid)]'invalid': ['['/][for (iv : ValueField | df.invalidValues) separator(', ')][if (df.dataType=DataType::String or df.dataType=DataType::Time or df.dataType=DataType::DateTime)]'[iv.value/]'[else][iv.value/][/if][/for][']'/] [elseif(spVal.specialType=SpecialValue::Outlier)]'outlier': True[/if][/for]}[/for]}

								[for (dd_out : DataDictionary | dataProcessing.outputPort)]				[comment OPEN traverse all the output DataDictionaries/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_filter_rows_special_values(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
																										cols_special_type_values=dicc_[fValue.filterValueDef.name/],
																										filter_type=FilterType([if (fValue.filterType=FilterType::EXCLUDE)]0[else]1[/if]))
								[/for]																	[comment CLOSE traverse all the output DataDictionaries/]
							[/if]																		[comment CLOSE checks that list of SpecialValues is not empty/]
						[/let]																											[comment CLOSE assigns to a set only the SpecialValues/]
						[let filteredSet : OrderedSet(Range) = fValue.matchingvalue->select(e | e.oclIsKindOf(Range))]					[comment OPEN assigns to a set only the Range/]
							[if (not filteredSet->isEmpty())]											[comment OPEN checks that the list of Range is not empty/]
filter_range_left_values_list_[fValue.filterValueDef.name/]=['['/][for (range : Range | filteredSet) separator(', ')][if (range.minInfinity=true)]-np.inf[else][range.min/][/if][/for][']'/]
filter_range_right_values_list_[fValue.filterValueDef.name/]=['['/][for (range : Range | filteredSet) separator(', ')][if (range.maxInfinity=true)]np.inf[else][range.max/][/if][/for][']'/]
closure_type_list_[fValue.filterValueDef.name/]=['['/][for (range : Range | filteredSet) separator(', ')]Closure([if (range.clousure=ClosureType::opeOpen)]0[elseif(range.clousure=ClosureType::openClosed)]1[elseif(range.clousure=ClosureType::closedOpen)]2[elseif(range.clousure=ClosureType::closedClosed)]3[/if])[/for][']'/]

								[for (dd_out : DataDictionary | dataProcessing.outputPort)]				[comment OPEN traverse all the output DataDictionaries/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_filter_rows_range(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
																										columns=columns_[fValue.filterValueDef.name/],
																										left_margin_list=filter_range_left_values_list_[fValue.filterValueDef.name/],
																										right_margin_list=filter_range_right_values_list_[fValue.filterValueDef.name/],
																										filter_type=FilterType([if (fValue.filterType=FilterType::EXCLUDE)]0[else]1[/if]),
																										closure_type_list=closure_type_list_[fValue.filterValueDef.name/])
								[/for]																	[comment CLOSE traverse all the output DataDictionaries/]
							[/if]																		[comment CLOSE checks that the list of Range is not empty/]
						[/let]																											[comment CLOSE assigns to a set only the Range/]
					[/if]																				[comment CLOSE checks that the list of MatchingValues is not empty/]
				[/let]	
				[for (dd_out : DataDictionary | dataProcessing.outputPort)]								[comment OPEN traverse all the output DataDictionaries/]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
					[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
						[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
							[if (dpd.oclIsKindOf(Library::Transformation))]
								[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[dd_out.fileName/]')	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[dd_out.fileName/]', sep='[dd_out.columnDelimiter/]', index_col=0)
								[/let]
							[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/], sep=[dd_out.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)
							[/if]
						[/let]
					[/if]
				[/for]																					[comment CLOSE traverse all the output DataDictionaries/]
			[/for]																						[comment CLOSE traverse all the input DataDictionaries/]
		[/if]																							[comment CLOSE checks that the parameter is a FilterValue/]
	[/if]																																	[comment CLOSE checks that the DataProcessing is rowFilter/]
	[if (dataProcessing.dataProcessingDefinition.name='columnFilter')]									[comment OPEN checks that the DataProcessing is columnFilter/]
		[if (p.oclIsKindOf(Field))]																		[comment OPEN checks that the parameter is a Field/]
			[let field : Field = p.oclAsType(Field)]													[comment OPEN assigns the variable to be a field/]
				[for (dd_in : DataDictionary | dataProcessing.inputPort)]								[comment OPEN traverse all the input DataDictionaries/]
					[for (dd_out : DataDictionary | dataProcessing.outputPort)]							[comment OPEN traverse all the output DataDictionaries/]
field_list_[field.fieldDef.name/]=['['/][for (df : DataField | field.dataField) separator (', ')]'[df.displayName/]'[/for][']'/]

[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_filter_columns(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
																columns=field_list_[field.fieldDef.name/], belong_op=Belong.[field.operator/])

[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
						[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
							[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
								[if (dpd.oclIsKindOf(Library::Transformation))]
									[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv('[dd_out.fileName/]')	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv('[dd_out.fileName/]', sep='[dd_out.columnDelimiter/]', index_col=0)
									[/let]
								[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df.to_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_csv([dd_out.name.replaceAll('[(),\\s]+', '_')/], sep=[dd_out.name.replaceAll('[(),\\s]+', '_')/]_sep, index_col=0)
								[/if]
							[/let]
						[/if]
					[/for]																				[comment CLOSE traverse all the output DataDictionaries/]
				[/for]																					[comment CLOSE traverse all the input DataDictionaries/]
			[/let]																						[comment CLOSE assigns the variable to be a field/]
		[/if]																							[comment CLOSE checks that the parameter is a Field/]
	[/if]																								[comment CLOSE checks that the DataProcessing is columnFilter/]
[/for]																									[comment CLOSE traverse all the parameters in the DataProcessing/]
[/if]																									[comment CLOSE checks that the DataProcessing is not a Mapping/]
[/if]																									[comment CLOSE checks that there are parameters in the DataProcessing/]
[/template]


[template public generateEnvironment(aWorkflow : Workflow)]
[if (aWorkflow.environment.oclIsKindOf(Portable))]
[generatePortable()/]
[elseif (aWorkflow.environment.oclIsKindOf(Local))]
[generateLocal()/]
[/if]

[file ('LocalStorage.txt', false, 'UTF-8')]
	[for (st : Storage | aWorkflow.environment.storage)]
		[if (st.oclIsKindOf(LocalStorage))]
			[let ls : LocalStorage = st.oclAsType(LocalStorage)]
Dataset path = [ls.path/]
			[/let]
		[/if]
	[/for]
[/file]

[file ('RemoteStorage.txt', false, 'UTF-8')]
	[for (st : Storage | aWorkflow.environment.storage)]
		[if (st.oclIsKindOf(RemoteStorage))]
			[let rs : RemoteStorage = st.oclAsType(RemoteStorage)]
				[if (rs.oclIsKindOf(AmazonS3))]
					[let s3 : AmazonS3 = rs.oclAsType(AmazonS3)]
AmazonS3Bucket = [s3.bucket/]
AmazonS3Key = [s3.key/]
AmazonS3Region = [s3.region/]
AmazonS3SecretId = [s3.secretId/]
					[/let]
				[elseif (rs.oclIsKindOf(AzureBlob))]
					[let ab : AzureBlob = rs.oclAsType(AzureBlob)]
AzureBlobContainer = [ab.container/]
AzureBlobBlobName = [ab.blobName/]
AzureBlobKeyVaultName = [ab.keyVaultName/]
AzureBlobSecretName = [ab.secretName/]
					[/let]
				[/if]
			[/let]
		[/if]
	[/for]
[/file]
[/template]


[template public generateLocal(aWorkflow : Workflow)]
[let lo : Local = aWorkflow.environment.oclAsType(Local)]
	[file ('Local.txt', false, 'UTF-8')]
EnvironmentName = [lo.name/]
EnvironmentPath = [lo.path/]
EnvironmentPythonInterpreter = [lo.pythonInterpreter.path/]
	[/file]
[/let]
[/template]


[template public generatePortable(aWorkflow : Workflow)]
[let por : Portable = aWorkflow.environment.oclAsType(Portable)]
	[if (por.container.oclIsTypeOf(Docker))]
[generateDocker()/]
	[elseif (por.container.oclIsTypeOf(AmazonECS))]
[generateAmazonECS()/]
	[elseif (por.container.oclIsTypeOf(AzureCI))]
[generateAzureCI()/]
	[/if]
[/let]
[/template]


[template public generateDocker(aWorkflow : Workflow)]
[let por : Portable = aWorkflow.environment.oclAsType(Portable)]
	[file ('Dockerfile', false, 'UTF-8')]
FROM python:[por.container.developmentTool.version/]-slim


WORKDIR [por.path/]


RUN apt-get update && \
    apt-get install -y --no-install-recommends git && \
    rm -rf /var/lib/apt/lists/*


RUN git clone --depth 1 --branch develop https://github.com/franjmelchor/MD4DSP-m2python.git .

RUN pip install --no-cache-dir -r requirements.txt


ENV PYTHONPATH=[por.path/]


		[for (dp : DataProcessing | aWorkflow.dataprocessing)]
COPY ./dataProcessing_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/].py [por.path/]/workflows/
COPY ./contracts_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/].py [por.path/]/workflows/
COPY ./transformations_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/].py [por.path/]/workflows/
		[/for]

COPY workflow_scripts.sh [por.path/]/workflow_scripts.sh

RUN chmod +x [por.path/]/workflow_scripts.sh

CMD ['['/]"/bin/bash", "[por.path/]/workflow_scripts.sh"[']'/]
	[/file]


	[file ('deploy_docker_app.sh', false, 'UTF-8')]
#!/bin/bash
set -e

sudo apt-get update --yes
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

echo \
  "deb ['['/]arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update --yes

sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin --yes

docker build -t [por.container.imageName.replaceAll('[(),\\s]+', '_')/]:[por.container.imageTag.replaceAll('[(),\\s]+', '_')/] -f Dockerfile .

clear

docker run -it --rm --name [por.container.name.replaceAll('[(),\\s]+', '_')/] --mount type=bind,source=[let ls : LocalStorage = por.storage->first().oclAsType(LocalStorage)][ls.path/][/let],target=[if (por.path.startsWith('.'))][por.path.replaceFirst('.', '')/][else][por.path/][/if]/data [por.container.imageName.replaceAll('[(),\\s]+', '_')/]:[por.container.imageTag.replaceAll('[(),\\s]+', '_')/]

docker rmi [por.container.imageName.replaceAll('[(),\\s]+', '_')/]:[por.container.imageTag.replaceAll('[(),\\s]+', '_')/]

clear

echo -e "Exiting the application...\n"
	[/file]
[/let]


[file ('workflow_scripts.sh', false, 'UTF-8')]
#CAMBIAR LAS RUTAS DE LOS COPY DE LOS ARCHIVOS GENERADOS

CONTRACTS_SCRIPT="workflows.contracts_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/]"
TRANSFORMATIONS_SCRIPT="workflows.transformations_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/]"
WORKFLOW_SCRIPT="workflows.dataProcessing_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/]"

while true; do
    echo -e "\nWhat would you like to do?"
    echo "    1. Execute the Workflow validation contracts"
    echo "    2. Execute the Workflow data transformations"
    echo "    3. Execute the complete Pipeline (transformations and contracts)"
    echo -e "    4. Exit\n"

    read -r -p "Select an option: " option
    clear

	if ['['/] "$option" -eq 1 [']'/]; then
        echo -e "Executing the Workflow validation contracts...\n"
        if ! python3 -m $CONTRACTS_SCRIPT; then
            echo "An error occurred while executing the Workflow validation contracts."
        fi
    elif ['['/] "$option" -eq 2 [']'/]; then
        echo -e "Executing the Workflow data transformations...\n"
        if ! python3 -m $TRANSFORMATIONS_SCRIPT; then
            echo "An error occurred while executing the Workflow data transformations."
        fi
    elif ['['/] "$option" -eq 3 [']'/]; then
        echo -e "Executing the complete Pipeline...\n"
        if ! python3 -m $WORKFLOW_SCRIPT; then
            echo "An error occurred while executing the complete Pipeline."
        fi
    elif ['['/] "$option" -eq 4 [']'/]; then
        break
    else
        echo -e "Invalid option. Please select a valid option.\n"
    fi
done
[/file]

[/template]


[template public generateAmazonECS(aWorkflow : Workflow)]
[let por : Portable = aWorkflow.environment.oclAsType(Portable)]
	[file ('AmazonECSfile', false, 'UTF-8')]
		[let ecs : AmazonECS = por.container.oclAsType(AmazonECS)]
SecretId = [ecs.secretId/]
Region = [ecs.region/]
Key = [ecs.key/]
		[/let]
	[/file]
[/let]
[/template]


[template public generateAzureCI(aWorkflow : Workflow)]
[let por : Portable = aWorkflow.environment.oclAsType(Portable)]
	[file ('AzureCIfile', false, 'UTF-8')]
		[let ci : AzureCI = por.container.oclAsType(AzureCI)]
AcrName = [ci.acrName/]
KeyVaultName = [ci.keyVaultName/]
SecretName = [ci.secretName/]
BlobName = [ci.blobName/]
		[/let]
	[/file]
[/let]

[/template]




















